{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "version_major": 2,
        "version_minor": 0,
        "state": {
        "001c221b24b644ebae9653b55f47ad32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3189e967dd3c486f8fd9e097017968b7",
              "IPY_MODEL_bb3dacc6ce9b49e7bdc9dcc34b750007",
              "IPY_MODEL_6dcbf5a61a1846e68abe7471bea3c826"
            ],
            "layout": "IPY_MODEL_3dd241fe845e4037862b71f683351c88"
          }
        },
        "3189e967dd3c486f8fd9e097017968b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "Baseline (NLP)",
              "LLM (OpenAI)"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Summarizer:",
            "description_tooltip": null,
            "disabled": false,
            "index": 1,
            "layout": "IPY_MODEL_be252ba814d34cc797be3c3849340594",
            "style": "IPY_MODEL_4061c4f09dad4051b1043be77bd9a715"
          }
        },
        "bb3dacc6ce9b49e7bdc9dcc34b750007": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "primary",
            "description": "Run",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_973c09eb3f5d49a89a0cbb0610c909dc",
            "style": "IPY_MODEL_043f55c3de354be0ae652057e22993c6",
            "tooltip": ""
          }
        },
        "6dcbf5a61a1846e68abe7471bea3c826": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_ad0beb684bb14b8c81f84dca37ebeb3b",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "Running with mode: llm...\n",
                  "Generating podcast audio... this can take a few minutes.Please keep this tab open. The audio player and download will appear below when ready.\n",
                  "\n",
                  "\n"
                ]
              },
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "Welcome! Today we're turning a dense PDF into an easy conversation.\n",
                  "We will break it down section by section, highlighting the key ideas. \n",
                  "\n",
                  "Section 1: Google / Models / Attention. Here's the gist. \n",
                  "\n",
                  "- Attention Is All You Need Ashish Vaswani∗ Google Brain Google Brain Google Research Google Research Llion Jones∗ Google Research Google Brain Illia Polosukhin∗ ‡ Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder.\n",
                  "- On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.\n",
                  "- Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works.\n",
                  "- Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU.\n",
                  "Nice. Let's keep moving. \n",
                  "Got it. On to the next part. \n",
                  "\n",
                  "Up next: Recurrent / Models / Codebase.\n",
                  "\n",
                  "Section 2: Recurrent / Models / Codebase. Here's the gist. \n",
                  "\n",
                  "- 1 Introduction Recurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [ 35,2,5].\n",
                  "- Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.\n",
                  "- Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail.\n",
                  "- Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15].\n",
                  "Nice. Let's keep moving. \n",
                  "Got it. On to the next part. \n",
                  "\n",
                  "Up next: Computation / Sequence / Input.\n",
                  "\n",
                  "Section 3: Computation / Sequence / Input. Here's the gist. \n",
                  "\n",
                  "- Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.\n",
                  "- Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [ 2,19].\n",
                  "- In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw gl \n",
                  "---\n",
                  "\n"
                ]
              },
              {
                "output_type": "error",
                "ename": "AuthenticationError",
                "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: OPENAI_A**_KEY. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
                "traceback": [
                  "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                  "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
                  "\u001b[0;32m/tmp/ipython-input-842760560.py\u001b[0m in \u001b[0;36mon_run\u001b[0;34m(_)\u001b[0m\n\u001b[1;32m     20\u001b[0m     print(f\"Generating podcast audio... this can take a few minutes.\"\n\u001b[1;32m     21\u001b[0m           \"Please keep this tab open. The audio player and download will appear below when ready.\\n\\n\")\n\u001b[0;32m---> 22\u001b[0;31m     audio_path = run_pipeline(pdf_filename, title = \"Podcast\", max_tokens= 250,\n\u001b[0m\u001b[1;32m     23\u001b[0m                               \u001b[0mout_basename\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"podcast\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"mp3\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                               summarizer_mode= mode_dd.value)\n",
                  "\u001b[0;32m/tmp/ipython-input-1999200575.py\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m(pdf_path, title, max_tokens, out_basename, fmt, summarizer_mode)\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mscript\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoin_dialogue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscript\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n---\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   audio_path = render_podcast_audio_by_section_from_outline(outline, title,\n\u001b[0m\u001b[1;32m     13\u001b[0m                                                             out_basename = out_basename, fmt = fmt)\n\u001b[1;32m     14\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0maudio_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m/tmp/ipython-input-1191510991.py\u001b[0m in \u001b[0;36mrender_podcast_audio_by_section_from_outline\u001b[0;34m(outline, title, out_basename, fmt)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mvoice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen_lines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{out_basename}_path{idx}.{fmt}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m       \u001b[0msynth_openai_tts_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPODCAST_OPEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m       \u001b[0mclips\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m       \u001b[0midx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m/tmp/ipython-input-1191510991.py\u001b[0m in \u001b[0;36msynth_openai_tts_to_file\u001b[0;34m(text, out_path, model, voice, fmt)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   with client.audio.speech.with_streaming_response.create(\n\u001b[0m\u001b[1;32m     19\u001b[0m       \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0mvoice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvoice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_response.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_APIResponseT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/audio/speech.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, input, model, voice, instructions, response_format, speed, stream_format, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[1;32m    102\u001b[0m         \u001b[0mextra_headers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"Accept\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"application/octet-stream\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextra_headers\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    104\u001b[0m             \u001b[0;34m\"/audio/speech\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             body=maybe_transform(\n",
                  "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n",
                  "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                  "\u001b[0;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: OPENAI_A**_KEY. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
                ]
              }
            ]
          }
        },
        "3dd241fe845e4037862b71f683351c88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be252ba814d34cc797be3c3849340594": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4061c4f09dad4051b1043be77bd9a715": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "973c09eb3f5d49a89a0cbb0610c909dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "043f55c3de354be0ae652057e22993c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "ad0beb684bb14b8c81f84dca37ebeb3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
            }
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ritikade2/PDF-to-Podcast/blob/main/PDF_to_Podcast.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pypdf2 nltk pydub openai\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"OPENAI_API_KEY\""
      ],
      "metadata": {
        "id": "yqaVmA5RQ_fr"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BH-slc9tIqnc",
        "outputId": "b73eb515-7ff3-4119-c023-6d4fd8803703"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "from typing import List, Tuple\n",
        "import PyPDF2\n",
        "# NLP\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from collections import Counter\n",
        "# OpenAI\n",
        "from openai import OpenAI\n",
        "# Audio\n",
        "from pydub import AudioSegment\n",
        "# Colab UI\n",
        "from IPython.display import Audio, display, clear_output\n",
        "from ipywidgets import Dropdown, Button, VBox, Output\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------\n",
        "# Config: Summarizer Toggle\n",
        "#-------------------------------------\n",
        "BASELINE_SUMMARY = 'nlp' # NLTK extractive\n",
        "LLM_SUMMARY = 'llm' # OpenAI LLM bullets\n",
        "DEFAULT_SUMMARIZER_MODE = BASELINE_SUMMARY # default is NLTK model. Switch to 'llm' for OpenAI\n",
        "LLM_MODEL_SUMMARY = 'gpt-4o-mini'"
      ],
      "metadata": {
        "id": "x_Y7MCq8tSvW"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up NLTK\n",
        "try:\n",
        "  nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "  nltk.download('punkt')"
      ],
      "metadata": {
        "id": "cKAjT3igLGeX"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------\n",
        "# Helper Functions\n",
        "#-------------------------------------\n",
        "\n",
        "#------- Reducing Noisy Text --------#\n",
        "\n",
        "# Drop any residual noisy sentences\n",
        "noisy_pat = re.compile(r\"(http[s]?://|\\bdoi:|arxiv|issn|isbn|©|all rights reserved)\", re.IGNORECASE)\n",
        "def is_noisy_text(s: str) -> bool:\n",
        "  return bool(noisy_pat.search(s or \"\"))\n",
        "\n",
        "# Reducing noise in header/body\n",
        "safe_noise_re = re.compile(\n",
        "    r\"(\"\n",
        "    r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9._-]+\\.[A-Za-z]{2,}\\b\"  # emails\n",
        "    r\"|https?://\\S+\" # links\n",
        "    r\"|\\bdoi:\\s*\\S+\" # doi\n",
        "    r\"|\\b(?:arxiv|issn|isbn)\\b[:\\s]*\\S+\" # ids\n",
        "    r\"|\\borcid\\.org/\\S+\" # orcid\n",
        "    r\"|(?:copyright|©|all rights reserved)\" # copyright\n",
        "    r\")\",\n",
        "    re.IGNORECASE,\n",
        "  )\n",
        "\n",
        "affiliation_hints = {'university', \"department\", \"school\", \"laboratory\",\n",
        "                     \"institute\", \"faculty\", \"college\", \"center\", \"centre\"}\n",
        "abstract_block = re.compile(\n",
        "    r\"\\b(?:abstract|summary)\\b\\s*[:.\\-]?\\s*(.+?)\"\n",
        "    r\"(?=\\n\\s*(?:keywords?|index terms?|introduction|1\\.|\\d\\.)\\b)\",\n",
        "    re.IGNORECASE | re.DOTALL,\n",
        ")\n",
        "kw_lines = re.compile(r\"^\\s*(?:keywords?|index terms?)\\b.*$\", re.IGNORECASE | re.MULTILINE)\n",
        "def strip_abstract_and_keywords(text: str) -> str:\n",
        "  t = re.sub(abstract_block, \"\", text)\n",
        "  t = re.sub(kw_lines, \"\", t)\n",
        "  return t\n",
        "\n",
        "def authorish_header_lines(s: str) -> bool:\n",
        "  st = s.strip()\n",
        "  if not st:\n",
        "    return True # ignore empty lines in header\n",
        "  score = 0\n",
        "  if safe_noise_re.search(st):\n",
        "    score += 2 # emails, urls, doi's, etc.\n",
        "  if any(w in st.lower() for w in affiliation_hints):\n",
        "    score += 2 # affiliations\n",
        "  if st.count(',') >= 3:\n",
        "    score += 1 # too many commas\n",
        "  if 20 <= len(st) <= 250:\n",
        "    score += 1 # short lines\n",
        "  if re.search(r\"[\\*\\u2020\\u2021]\", st):\n",
        "    score += 1 # footnotes\n",
        "  return score >= 3\n",
        "\n",
        "# Remove author/affiliation noise in header and obvious noise.\n",
        "def preclean_text(text: str, header_lines: int = 80) -> str:\n",
        "  lines = text.splitlines()\n",
        "  abstract_idx = None\n",
        "  for i, ln in enumerate(lines):\n",
        "    if re.search(r\"\\b(abstract|summary)\\b[:\\s]\", ln, flags= re.IGNORECASE):\n",
        "      abstract_idx = i\n",
        "      break\n",
        "\n",
        "  cleaned = []\n",
        "  for i, ln in enumerate(lines):\n",
        "    in_header = (abstract_idx is None and i < header_lines) or (abstract_idx is not None and i < abstract_idx)\n",
        "    if in_header:\n",
        "      if authorish_header_lines(ln):\n",
        "        continue\n",
        "      if safe_noise_re.search(ln):\n",
        "        continue\n",
        "    else:\n",
        "      # body: only strip obvious noise\n",
        "      if safe_noise_re.search(ln):\n",
        "        continue\n",
        "    cleaned.append(ln)\n",
        "  out = re.sub(r\"\\s+\", \" \", \"\\n\".join(cleaned)).strip()\n",
        "  out = strip_abstract_and_keywords(out)\n",
        "  return out\n",
        "\n",
        "\n",
        "#------- PDF Extraction Helper --------#\n",
        "def extract_text_from_pdf(pdf_path: str) -> str:\n",
        "  reader = PyPDF2.PdfReader(pdf_path)\n",
        "  pages = []\n",
        "  for pg in reader.pages:\n",
        "    text = pg.extract_text() or \"\"\n",
        "    pages.append(text)\n",
        "  all_text = \"\\n\\n\".join(pages)\n",
        "  return preclean_text(all_text)\n",
        "\n",
        "\n",
        "#------- Section & Title Helpers --------#\n",
        "\n",
        "# Titles\n",
        "def guess_title(chunk_text: str)-> str:\n",
        "  chunk_text = preclean_text(chunk_text)\n",
        "  words = [w.lower() for w in word_tokenize(chunk_text) if re.match(r\"[A-Za-z\\-]+$\", w)]\n",
        "  stopwords = set(\"\"\"a an the and or to of for with on at by from is are\n",
        "  was were be been being into that this those these which who whom whose\n",
        "  in out over under as it he she they we you i their our your its\"\"\".split())\n",
        "  filtered_words = [w for w in words if w not in stopwords and len(w) > 3]\n",
        "  freq = Counter(filtered_words)\n",
        "  top = [w.capitalize() for w,_ in freq.most_common(3)]\n",
        "  return \" / \".join(top) if top else \"Section\"\n",
        "\n",
        "# Section Text\n",
        "def split_into_sections(text:str, max_tokens: int = 900) -> List[Tuple[str,str]]:\n",
        "  sentences = sent_tokenize(text)\n",
        "  chunks, current_chunk, token_count = [], [], 0\n",
        "  for sentence in sentences:\n",
        "    words = word_tokenize(sentence)\n",
        "    # if adding this sentence exceeds the cap, flush the remaining\n",
        "    if token_count + len(words) > max_tokens and current_chunk:\n",
        "      chunk_text = \" \".join(current_chunk).strip()\n",
        "      chunks.append((guess_title(chunk_text), chunk_text))\n",
        "      current_chunk, token_count = [], 0\n",
        "    current_chunk.append(sentence)\n",
        "    token_count += len(words)\n",
        "  # final flush of text to chuck\n",
        "  if current_chunk:\n",
        "    chunk_text = \" \".join(current_chunk).strip()\n",
        "    chunks.append((guess_title(chunk_text), chunk_text))\n",
        "  return chunks\n"
      ],
      "metadata": {
        "id": "8bgxJEifSlgv"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#--------------------------------------\n",
        "# Summarizers\n",
        "#--------------------------------------\n",
        "\n",
        "#------- Baseline NLP Summarizer --------#\n",
        "\n",
        "# Identify Keywords\n",
        "def keyword_scores(text: str) -> Counter:\n",
        "  words = [w.lower() for w in word_tokenize(text) if re.match(r'[A-Za-z\\-]+$', w)]\n",
        "  stop = set(\"\"\"a an the and or to of for with on at by from is are was were be\n",
        "  been being into that this those these which who whom whose in out over\n",
        "  under as it he she they we you i their our your its\"\"\".split())\n",
        "  filtered_words = [w for w in words if w not in stop and len(w)>3]\n",
        "  return Counter(filtered_words)\n",
        "\n",
        "# Summarize using NLP\n",
        "def extractive_bullets(text: str, k: int = 4) -> List[str]:\n",
        "  cleaned_text = preclean_text(text)\n",
        "  sentences = [s for s in sent_tokenize(cleaned_text) if not is_noisy_text(s)]\n",
        "  if not sentences:\n",
        "    return []\n",
        "  kw = keyword_scores(text)\n",
        "  scored = []\n",
        "  # Scoring keywords in sentences\n",
        "  for i, s in enumerate(sentences):\n",
        "    sw = [w.lower() for w in word_tokenize(s) if re.match(r'[A-Za-z\\-]+$', w)]\n",
        "    score = sum(kw[w] for w in sw)\n",
        "    scored.append((score, i, s))\n",
        "  scored.sort(reverse=True)\n",
        "\n",
        "  top = [s for _, _, s in scored[:k]]\n",
        "  seen, uniq = set(), []\n",
        "  # Return final bullet points using sentences with top keywords\n",
        "  for b in top:\n",
        "    key = re.sub(r\"[^a-z]\", \"\", b.lower())[:60]\n",
        "    if key not in seen:\n",
        "      seen.add(key)\n",
        "      uniq.append(b.strip())\n",
        "  return uniq\n",
        "\n",
        "\n",
        "#------- LLM extractive bullets (OpenAI) --------#\n",
        "\n",
        "def llm_bullets(text: str, k: int = 4, model: str = LLM_MODEL_SUMMARY) -> List[str]:\n",
        "  cleaned = preclean_text(text)\n",
        "  try:\n",
        "    client = OpenAI()\n",
        "    prompt = (\n",
        "        f\"Summarize the following section into {k} concise, self-contained bullet sentences.\\n\"\n",
        "        \"Ignore author names, affiliations, emails, URLs, ORCIDs, DOIs, and any 'Abstract', 'Summary', 'Keywords', or 'References' sections.\\n\"\n",
        "        \"Do not add headers; return each bullet on a new line.\\n\\nSECTION:\\n\" + cleaned\n",
        "    )\n",
        "    resp = client.responses.create(\n",
        "        model = model,\n",
        "        input = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    # Try to extract plain text\n",
        "    try:\n",
        "      txt = resp.output_text().strip()\n",
        "    except Exception:\n",
        "        # Fallback parse\n",
        "        txt = \"\\n\".join(\n",
        "          [c.get('text','') for o in getattr(resp, 'output', []) for c in getattr(o, 'content', []) if isinstance(c, dict)]\n",
        "        ).strip()\n",
        "    lines = [re.sub(r\"^[\\-\\*\\d\\.)\\s]+\",\"\", ln).strip() for ln in (txt.splitlines() if txt else []) if ln.strip()]\n",
        "    return lines[:k] if lines else []\n",
        "  except Exception:\n",
        "    # Fallback to baseline method\n",
        "    return extractive_bullets(cleaned, k=k)\n",
        "\n",
        "# Wrapper to choose summarizer\n",
        "def summarize_to_bullets(text: str, k:int = 4, mode: str = DEFAULT_SUMMARIZER_MODE) -> List[str]:\n",
        "  clean_txt = preclean_text(text)\n",
        "  if (mode or '').lower() == LLM_SUMMARY:\n",
        "    return llm_bullets(clean_txt, k= k)\n",
        "  else:\n",
        "    extractive_bullets(clean_txt, k=k)"
      ],
      "metadata": {
        "id": "xWaVDbec5rW9"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#--------------------------------------\n",
        "# Building Outlines\n",
        "#--------------------------------------\n",
        "\n",
        "# Sections\n",
        "def make_section(title:str, text:str, bullets:List[str]) -> dict:\n",
        "  return{\"title\": title, \"text\": text, \"bullets\": bullets}\n",
        "\n",
        "# Outlines: Takes raw sections (title + text) and builds clean outlines with bullet summaries\n",
        "def build_outline (sections_raw: List[Tuple[str, str]], summarizer_mode: str = DEFAULT_SUMMARIZER_MODE) -> List[dict]:\n",
        "  outline = []\n",
        "  for title, chunk in sections_raw:\n",
        "    cleaned_chunk = preclean_text(chunk) # remove noise\n",
        "    bullets = extractive_bullets(cleaned_chunk, k = 4) # generate bullet summaries\n",
        "    section_entry = make_section(title, cleaned_chunk, bullets) # Build structured dict for this section\n",
        "    outline.append(section_entry) # append to final outline\n",
        "  return outline\n"
      ],
      "metadata": {
        "id": "QO56nVNwHtvK"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#--------------------------------------\n",
        "# Building Script\n",
        "#--------------------------------------\n",
        "\n",
        "# Podcast Operners & Closers\n",
        "PODCAST_OPEN = (\n",
        "    \"Welcome! Today we're turning a dense PDF into an easy conversation.\\n\"\n",
        "    \"We will break it down section by section, highlighting the key ideas. \\n\"\n",
        ")\n",
        "\n",
        "PODCAST_CLOSE = (\n",
        "    \"That's a wrap. If you enjoyed this, share it and read the full paper.\\n\"\n",
        "    \"Thanks for listening. See you next time!\\n\"\n",
        ")\n",
        "\n",
        "# Dialogue\n",
        "def template_dialogue_for_section(idx: int, sec: dict, next_title: str = None) -> str:\n",
        "  lead = f\"Section {idx+1}: {sec['title']}. Here's the gist. \\n\"\n",
        "  lines = [lead]\n",
        "  for b in sec['bullets']:\n",
        "    lines.append(f\"- {b}\")\n",
        "  # Transition lines\n",
        "  transitions = [\n",
        "      \"Nice. Let's keep moving. \\n\"\n",
        "      \"Got it. On to the next part. \\n\"\n",
        "  ]\n",
        "  lines.append(transitions[idx % len(transitions)])\n",
        "  if next_title:\n",
        "    lines.append(f\"Up next: {next_title}.\\n\")\n",
        "  return \"\\n\".join(lines)\n",
        "\n",
        "# Script\n",
        "def join_dialogue(podcast_title: str, outline: List[dict]) -> str:\n",
        "  parts = [PODCAST_OPEN]\n",
        "  for i, sec in enumerate(outline):\n",
        "    nxt = outline[i+1][\"title\"] if i < len(outline) - 1 else None\n",
        "    parts.append(template_dialogue_for_section(i, sec, next_title= nxt))\n",
        "  parts.append(PODCAST_CLOSE)\n",
        "  return \"\\n\".join(parts)"
      ],
      "metadata": {
        "id": "ePr5XCX_ODvA"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#--------------------------------------\n",
        "# Audio (TTS and Stitching)\n",
        "#--------------------------------------\n",
        "HOST1_VOICE = \"alloy\"\n",
        "HOST2_VOICE = \"verse\"\n",
        "\n",
        "def stitch_audio(files: List[str], out_path: str, fmt: str = \"mp3\", gap_ms: int = 350):\n",
        "  combined = AudioSegment.silent(duration=0)\n",
        "  for f in files:\n",
        "    seg = AudioSegment.from_file(f)\n",
        "    combined += seg + AudioSegment.silent(duration= gap_ms)\n",
        "  combined.export(out_path, format= fmt)\n",
        "\n",
        "def synth_openai_tts_to_file(text: str, out_path: str, model: str = \"gpt-4o-mini-tts\", voice: str = \"alloy\", fmt: str = \"mp3\"):\n",
        "  if not text or not text.strip():\n",
        "    return\n",
        "  client = OpenAI()\n",
        "  with client.audio.speech.with_streaming_response.create(\n",
        "      model = model,\n",
        "      voice = voice,\n",
        "      input = text,\n",
        "      response_format = fmt\n",
        "  ) as response: response.stream_to_file(out_path)\n",
        "\n",
        "\n",
        "def render_podcast_audio_by_section_from_outline(outline: list, title: str, out_basename: str = \"podcast\", fmt: str = \"mp3\") -> str:\n",
        "    clips, idx = [], 0\n",
        "    # Opening\n",
        "    open_lines = [\n",
        "        (HOST1_VOICE, \"Welcome! Today we're turning a dense PDF into an easy conversation.\"),\n",
        "        (HOST2_VOICE, \"We will break it down section by section, highlighting the key ideas.\"),\n",
        "    ]\n",
        "    for voice, line in open_lines:\n",
        "      p = f\"{out_basename}_path{idx}.{fmt}\"\n",
        "      synth_openai_tts_to_file(PODCAST_OPEN, p, fmt= fmt)\n",
        "      clips.append(p)\n",
        "      idx += 1\n",
        "\n",
        "    # Sections\n",
        "    for i, sec in enumerate(outline):\n",
        "      lead_voice = HOST1_VOICE if i % 2 == 0 else HOST2_VOICE\n",
        "      p = f\"{out_basename}_path{idx}.{fmt}\"\n",
        "      synth_openai_tts_to_file(f\"Section {i+1}: {sec['title']}. Here's the gist\", p,voice = lead_voice, fmt= fmt)\n",
        "      clips.append(p)\n",
        "      idx += 1\n",
        "\n",
        "      for j, b in enumerate(sec.get(\"bullets\", [])):\n",
        "        voice = HOST2_VOICE if (i+j) % 2 == 0 else HOST1_VOICE\n",
        "        p = f\"{out_basename}_path{idx}.{fmt}\"\n",
        "        synth_openai_tts_to_file(f\"- {b}\", p, voice = voice, fmt= fmt)\n",
        "        clips.append(p)\n",
        "        idx += 1\n",
        "\n",
        "      transitions = [\n",
        "          \"Nice. Let's keep moving.\\n\",\n",
        "          \"Got it. On to the next part.\\n\"\n",
        "      ]\n",
        "      trans_voice = HOST1_VOICE if i % 2 == 0 else HOST2_VOICE\n",
        "      p = f\"{out_basename}_path{idx}.{fmt}\"\n",
        "      synth_openai_tts_to_file(transitions[i % len(transitions)], p, voice = trans_voice, fmt = fmt)\n",
        "      clips.append(p)\n",
        "      idx += 1\n",
        "\n",
        "      if i < len(outline) - 1:\n",
        "        next_voice = HOST2_VOICE if (i+1) % 2 == 0 else HOST1_VOICE\n",
        "        p = f\"{out_basename}_path{idx}.{fmt}\"\n",
        "        synth_openai_tts_to_file(f\"Up next: {outline[i+1]['title']}.\", p, voice = next_voice, fmt = fmt)\n",
        "        clips.append\n",
        "        idx += 1\n",
        "\n",
        "    # Closing\n",
        "    close_lines = [\n",
        "        (HOST1_VOICE, \"That's a wrap. If you enjoyed this, share it and read the full paper.\"),\n",
        "        (HOST2_VOICE, \"Thanks for listening. See you next time!\"),\n",
        "    ]\n",
        "    for voice, line in close_lines:\n",
        "      p = f\"{out_basename}_path{idx}.{fmt}\"\n",
        "      synth_openai_tts_to_file(line, p, voice = voice, fmt = fmt)\n",
        "    clips.append(p)\n",
        "    idx += 1\n",
        "\n",
        "    # Final Stitch\n",
        "    final_path = f\"{out_basename}_full.{fmt}\"\n",
        "    stitch_audio(clips, final_path, fmt=fmt, gap_ms=350)\n",
        "    return final_path\n"
      ],
      "metadata": {
        "id": "M_QhNeyv5pmh"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#--------------------------------------\n",
        "# Driver\n",
        "#--------------------------------------\n",
        "def run_pipeline(pdf_path: str, title: str = \"Podcast\", max_tokens: int = 250,\n",
        "                 out_basename: str = \"podcast\", fmt: str = \"mp3\",\n",
        "                 summarizer_mode: str = DEFAULT_SUMMARIZER_MODE) -> str:\n",
        "  raw = extract_text_from_pdf(pdf_path)\n",
        "  sections = split_into_sections(raw, max_tokens = max_tokens)\n",
        "  outline = build_outline(sections, summarizer_mode= summarizer_mode)\n",
        "  script = join_dialogue(title, outline)\n",
        "  print(script[:3000], \"\\n---\\n\")\n",
        "  audio_path = render_podcast_audio_by_section_from_outline(outline, title,\n",
        "                                                            out_basename = out_basename, fmt = fmt)\n",
        "  return audio_path"
      ],
      "metadata": {
        "id": "ECiIU7j95pdO"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#--------------------------------------\n",
        "# File Upload + UI\n",
        "#--------------------------------------\n",
        "\n",
        "# Upload PDF\n",
        "uploaded = files.upload()\n",
        "pdf_filename = list(uploaded.keys())[0]\n",
        "mode_dd = Dropdown(\n",
        "    options = [('Baseline (NLP)', 'nlp'), ('LLM (OpenAI)', 'llm')],\n",
        "    value = 'nlp',\n",
        "    description = 'Summarizer:'\n",
        "    )\n",
        "run_button = Button(description = 'Run', button_style = 'primary')\n",
        "out = Output()\n",
        "\n",
        "def on_run(_):\n",
        "  with out:\n",
        "    clear_output()\n",
        "    print(f\"Running with mode: {mode_dd.value}...\")\n",
        "    print(f\"Generating podcast audio... this can take a few minutes.\"\n",
        "          \"Please keep this tab open. The audio player and download will appear below when ready.\\n\\n\")\n",
        "    audio_path = run_pipeline(pdf_filename, title = \"Podcast\", max_tokens= 250,\n",
        "                              out_basename= \"podcast\", fmt= \"mp3\",\n",
        "                              summarizer_mode= mode_dd.value)\n",
        "    display(Audio(filename= audio_path, autoplay= False))\n",
        "    files.download(audio_path)\n",
        "\n",
        "run_button.on_click(on_run)\n",
        "display(VBox([mode_dd, run_button, out]))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "001c221b24b644ebae9653b55f47ad32",
            "3189e967dd3c486f8fd9e097017968b7",
            "bb3dacc6ce9b49e7bdc9dcc34b750007",
            "6dcbf5a61a1846e68abe7471bea3c826",
            "3dd241fe845e4037862b71f683351c88",
            "be252ba814d34cc797be3c3849340594",
            "4061c4f09dad4051b1043be77bd9a715",
            "973c09eb3f5d49a89a0cbb0610c909dc",
            "043f55c3de354be0ae652057e22993c6",
            "ad0beb684bb14b8c81f84dca37ebeb3b"
          ]
        },
        "id": "_xDt-Hr-5pUH",
        "outputId": "8a2965f9-7986-4d99-a49a-8a85968cf576"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2dc33148-2d9b-4d94-b2ab-8a904605ef83\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2dc33148-2d9b-4d94-b2ab-8a904605ef83\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 1706.03762v7.pdf to 1706.03762v7 (1).pdf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Dropdown(description='Summarizer:', options=(('Baseline (NLP)', 'nlp'), ('LLM (OpenAI)', 'llm')…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "001c221b24b644ebae9653b55f47ad32"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K-UQl_ZZMOQZ"
      },
      "execution_count": 33,
      "outputs": []
    }
  ]
}
